# -Repo-Name-spark-transaction-optimizer
â€œEnd-to-end Spark optimization project processing 10 million synthetic transactions with 16 performance techniques, including pushdowns, broadcast joins, AQE, and partition tuning.â€

# Spark Transaction Optimizer ğŸš€

This project demonstrates a production-style Spark optimization pipeline handling:

- **10 million synthetic transactions**
- **5 million synthetic customers**
- **10,000 synthetic products**

It applies **16 critical Spark performance techniques** to show measurable improvements, including:

âœ… Column pruning  
âœ… Predicate and filter pushdown  
âœ… Project pushdown  
âœ… Sorting for join keys  
âœ… Efficient file formats (Parquet)  
âœ… WholeStage code generation  
âœ… Adaptive Query Execution (AQE)  
âœ… Kryo serialization  
âœ… Broadcast joins for small dimension tables  
âœ… Avoiding UDFs  
âœ… Minimizing data movement with repartitioning  
âœ… Tuning shuffle partitions  
âœ… Avoiding collect on large datasets  
âœ… DataFrame reuse with caching  
âœ… Writing data with optimal partition sizing  
âœ… Minimizing shuffle

The entire pipeline is fully **reproducible** in Google Colab or Databricks.

---

## ğŸ“ Project Structure

- `01_data_generation.py`: creates the synthetic datasets (customers, products, transactions)  
- `02_baseline.py`: runs the baseline query performance  
- `03_optimizations.py`: applies the 16 optimizations step by step  
- `README.md`: project overview  
- `final_sales_data/`: final optimized Parquet data

---

## ğŸš€ Key Technologies

- Apache Spark (PySpark)  
- Parquet  
- Google Colab / Databricks  

---

## ğŸ“Š Results

- Baseline execution time
- Optimized execution time with column pruning and broadcast joins
- Final partitioned data
- Overall performance improvement documented in the Spark UI

---

## ğŸ“Œ Usage

```bash
# run from Google Colab
!pip install pyspark
