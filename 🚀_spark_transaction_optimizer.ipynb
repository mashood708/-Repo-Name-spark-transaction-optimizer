{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#🟢  Create Optimized Spark Session\n",
        "\n"
      ],
      "metadata": {
        "id": "ogbCn5chH0lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "import time\n",
        "\n",
        "# Spark session with Kryo serialization + shuffle partitions\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark Optimization Project\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "CpBH9FGYH_k_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🟢 : Generate Synthetic Data\n",
        "✅ 10 million transactions\n",
        "✅ 5 million customers\n",
        "✅ 10,000 products\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fnlZVn2eIQD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customer Dataset (5 million)\n"
      ],
      "metadata": {
        "id": "sSDqAeoUR2O5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_count = 5_000_000\n",
        "customers = spark.range(1, customer_count + 1) \\\n",
        "    .withColumnRenamed(\"id\", \"customer_id\") \\\n",
        "    .withColumn(\"name\", concat(lit(\"Customer_\"), col(\"customer_id\"))) \\\n",
        "    .withColumn(\"gender\", expr(\"CASE WHEN customer_id % 2 = 0 THEN 'M' ELSE 'F' END\")) \\\n",
        "    .withColumn(\"age\", (rand() * 50 + 18).cast(\"integer\")) \\\n",
        "    .withColumn(\"location\", expr(\"CASE WHEN customer_id % 5 = 0 THEN 'CityA' ELSE 'CityB' END\"))\n"
      ],
      "metadata": {
        "id": "n-P8D1V3IU8n"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Product Dataset (10,000)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Uvz3arkuR7ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product_count = 10_000\n",
        "products = spark.range(1, product_count + 1) \\\n",
        "    .withColumnRenamed(\"id\", \"product_id\") \\\n",
        "    .withColumn(\"product_name\", concat(lit(\"Product_\"), col(\"product_id\"))) \\\n",
        "    .withColumn(\"category\", expr(\"CASE WHEN product_id % 5 = 0 THEN 'CategoryA' ELSE 'CategoryB' END\")) \\\n",
        "    .withColumn(\"price\", (rand() * 100 + 5).cast(\"decimal(10,2)\"))\n"
      ],
      "metadata": {
        "id": "zF2sZSv_SChg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transaction Dataset (10 million)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gO9wQw8WSKlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
        "\n",
        "# get current timestamp in seconds\n",
        "current_ts = unix_timestamp(current_timestamp())\n",
        "\n",
        "# random seconds offset within the last year\n",
        "random_offset = (rand() * 365 * 24 * 60 * 60).cast(\"integer\")\n",
        "\n",
        "transactions = spark.range(1, transaction_count + 1) \\\n",
        "    .withColumnRenamed(\"id\", \"transaction_id\") \\\n",
        "    .withColumn(\"customer_id\", (rand() * customer_count).cast(\"integer\") + 1) \\\n",
        "    .withColumn(\"product_id\", (rand() * product_count).cast(\"integer\") + 1) \\\n",
        "    .withColumn(\"amount\", (rand() * 200 + 1).cast(\"decimal(10,2)\")) \\\n",
        "    .withColumn(\n",
        "        \"timestamp\",\n",
        "        from_unixtime(current_ts - random_offset)\n",
        "    ) \\\n",
        "    .withColumn(\n",
        "        \"payment_type\",\n",
        "        expr(\"CASE WHEN transaction_id % 3 = 0 THEN 'CARD' ELSE 'CASH' END\")\n",
        "    )\n"
      ],
      "metadata": {
        "id": "QN5oi9fEj3tU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#📁 3. Save to Parquet for Pushdown\n",
        "\n"
      ],
      "metadata": {
        "id": "vMn2Df6LkwrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers.write.mode(\"overwrite\").parquet(\"customers_parquet\")\n",
        "products.write.mode(\"overwrite\").parquet(\"products_parquet\")\n",
        "transactions.write.mode(\"overwrite\").parquet(\"transactions_parquet\")\n"
      ],
      "metadata": {
        "id": "sYIK8j1ES7cs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#📁 4. Baseline Performance\n",
        "\n"
      ],
      "metadata": {
        "id": "s1gPnfvblDYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers_df = spark.read.parquet(\"customers_parquet\")\n",
        "products_df = spark.read.parquet(\"products_parquet\")\n",
        "transactions_df = spark.read.parquet(\"transactions_parquet\")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "joined = transactions_df.join(customers_df, \"customer_id\", \"inner\") \\\n",
        "    .join(products_df, \"product_id\", \"inner\")\n",
        "\n",
        "baseline_result = joined.groupBy(\"category\").agg(sum(\"amount\").alias(\"total_revenue\"))\n",
        "baseline_result.show()\n",
        "\n",
        "print(f\"Baseline time: {time.time()-start:.2f} sec\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIM0TTZ3SOLE",
        "outputId": "148a13b5-053a-4693-b14b-aaefd270e8e7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+\n",
            "| category|total_revenue|\n",
            "+---------+-------------+\n",
            "|CategoryB| 807905245.81|\n",
            "|CategoryA| 202004432.78|\n",
            "+---------+-------------+\n",
            "\n",
            "Baseline time: 17.28 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "📁 5. Optimizations Step by Step\n"
      ],
      "metadata": {
        "id": "7hO3vociIyFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.1 Column Pruning\n"
      ],
      "metadata": {
        "id": "yl18K3z0lcrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n"
      ],
      "metadata": {
        "id": "977oCVQpmyC5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_pruned = spark.read.parquet(\"transactions_parquet\").select(\"customer_id\", \"product_id\", \"amount\")\n",
        "products_pruned = spark.read.parquet(\"products_parquet\").select(\"product_id\", \"category\")\n",
        "customers_pruned = spark.read.parquet(\"customers_parquet\").select(\"customer_id\")\n",
        "\n",
        "start = time.time()\n",
        "joined_pruned = transactions_pruned.join(customers_pruned, \"customer_id\", \"inner\") \\\n",
        "    .join(products_pruned, \"product_id\", \"inner\") \\\n",
        "    .groupBy(\"category\").agg(sum(\"amount\").alias(\"total_revenue\"))\n",
        "joined_pruned.show()\n",
        "print(f\"Column pruning time: {time.time()-start:.2f} sec\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_6zcJKuI15U",
        "outputId": "369e64f2-ce32-4788-801d-2e97f64fe87f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+\n",
            "| category|total_revenue|\n",
            "+---------+-------------+\n",
            "|CategoryB| 807905245.81|\n",
            "|CategoryA| 202004432.78|\n",
            "+---------+-------------+\n",
            "\n",
            "Column pruning time: 41.99 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.2 Predicate Pushdown\n"
      ],
      "metadata": {
        "id": "sV9xYyLOnTZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# filter high-value transactions at read\n",
        "transactions_filtered = spark.read.parquet(\"transactions_parquet\").select(\"customer_id\", \"product_id\", \"amount\") \\\n",
        "    .filter(col(\"amount\") > 50)\n",
        "\n",
        "start = time.time()\n",
        "joined_filtered = transactions_filtered.join(customers_pruned, \"customer_id\", \"inner\") \\\n",
        "    .join(products_pruned, \"product_id\", \"inner\") \\\n",
        "    .groupBy(\"category\").agg(sum(\"amount\").alias(\"total_revenue\"))\n",
        "joined_filtered.show()\n",
        "print(f\"Predicate pushdown time: {time.time()-start:.2f} sec\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzUTkwVUnNZp",
        "outputId": "01c95f06-327e-497e-f87e-df9f966d3cb5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+\n",
            "| category|total_revenue|\n",
            "+---------+-------------+\n",
            "|CategoryB| 757925086.09|\n",
            "|CategoryA| 189503719.52|\n",
            "+---------+-------------+\n",
            "\n",
            "Predicate pushdown time: 29.08 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.3 Filter Pushdown\n",
        "(same as predicate pushdown above, using filters at data scan stage — Spark handles them similarly)\n",
        "\n"
      ],
      "metadata": {
        "id": "b-U7Z5qlnoHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.4 Project Pushdown\n",
        "\n"
      ],
      "metadata": {
        "id": "apvd8y3Jn-cD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# already done via .select() in column pruning, no extra step\n"
      ],
      "metadata": {
        "id": "oZ-mP2H3n-AP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.5 Sorting for Joins\n",
        "\n"
      ],
      "metadata": {
        "id": "vDOd2SiooJK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try sorting product_id to improve shuffle\n",
        "transactions_sorted = transactions_pruned.sort(\"product_id\")\n",
        "# note: if join keys are skewed, bucketing is better\n"
      ],
      "metadata": {
        "id": "vv4EK8E2nxxv"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.6 Efficient File Formats\n",
        "✅ Already using Parquet (efficient, supports predicate + column pushdown).\n",
        "\n"
      ],
      "metadata": {
        "id": "vBQ_FqSVoUln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.7 WholeStage Code Generation\n"
      ],
      "metadata": {
        "id": "LeVJvXLYodRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n",
        "# default ON, just to confirm\n"
      ],
      "metadata": {
        "id": "BCp3VEzWokvs"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.8 Adaptive Query Execution (AQE)\n",
        "\n"
      ],
      "metadata": {
        "id": "t3MzMwINomxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n"
      ],
      "metadata": {
        "id": "cQiIIdUior-G"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.9 Kryo Serialization\n"
      ],
      "metadata": {
        "id": "jSVX0JmtowFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark Optimization Project\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "P3CsD7gbpRRH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.10 Avoiding Shuffles\n",
        "\n"
      ],
      "metadata": {
        "id": "FCKi5M2wpgfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use broadcast join for small products\n",
        "from pyspark.sql.functions import broadcast\n",
        "joined_broadcast = transactions_pruned.join(broadcast(products_pruned), \"product_id\", \"inner\")\n"
      ],
      "metadata": {
        "id": "G5zCecDbpWzy"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.11 Avoiding UDFs\n"
      ],
      "metadata": {
        "id": "LWdzV8RXpn4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_broadcast.withColumn(\"discounted\", col(\"amount\") * 0.9)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XoBKuWao1oD",
        "outputId": "7409b964-465b-4869-f822-c76da26a4b08"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[product_id: int, customer_id: int, amount: decimal(10,2), category: string, discounted: double]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.12 Minimizing Data Movement\n",
        "\n"
      ],
      "metadata": {
        "id": "u2OI3YGJpwiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# partition transactions on customer_id to colocate\n",
        "transactions_partitioned = transactions_pruned.repartition(\"customer_id\")\n"
      ],
      "metadata": {
        "id": "G0LFRH_Up18W"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.13 Tuning spark.sql.shuffle.partitions\n",
        "\n"
      ],
      "metadata": {
        "id": "9xIxypihp6SD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"400\")\n"
      ],
      "metadata": {
        "id": "kcqH9wHsp_Tq"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.14 Avoid collect() on Large Data\n"
      ],
      "metadata": {
        "id": "gU3H0j6tqELY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dangerous:\n",
        "# data = transactions_pruned.collect()\n",
        "transactions_pruned.show(5)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dt02pxoqLpU",
        "outputId": "fb761262-c9ed-476c-c8c5-8b9cf9e3e443"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+------+\n",
            "|customer_id|product_id|amount|\n",
            "+-----------+----------+------+\n",
            "|    2315473|      5129|  3.65|\n",
            "|    3126779|       114| 93.97|\n",
            "|    3500607|      8312| 42.01|\n",
            "|    2761934|      1992| 42.79|\n",
            "|    2206611|      2092|197.16|\n",
            "+-----------+----------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_pruned.take(10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cYm5Vc_qewp",
        "outputId": "786fa49e-2641-4f31-fa93-9f73e0e812dc"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(customer_id=2315473, product_id=5129, amount=Decimal('3.65')),\n",
              " Row(customer_id=3126779, product_id=114, amount=Decimal('93.97')),\n",
              " Row(customer_id=3500607, product_id=8312, amount=Decimal('42.01')),\n",
              " Row(customer_id=2761934, product_id=1992, amount=Decimal('42.79')),\n",
              " Row(customer_id=2206611, product_id=2092, amount=Decimal('197.16')),\n",
              " Row(customer_id=798749, product_id=2618, amount=Decimal('152.06')),\n",
              " Row(customer_id=4019106, product_id=9297, amount=Decimal('103.20')),\n",
              " Row(customer_id=441124, product_id=1984, amount=Decimal('92.07')),\n",
              " Row(customer_id=1452174, product_id=8459, amount=Decimal('51.02')),\n",
              " Row(customer_id=489062, product_id=2879, amount=Decimal('152.05'))]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.15 Reuse DataFrames\n",
        "\n"
      ],
      "metadata": {
        "id": "pZy9HGxatlVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_cached = transactions_pruned.cache()\n",
        "# reused in multiple joins or queries\n"
      ],
      "metadata": {
        "id": "28LQtyF5toph"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.16 Writing Data with Optimal Partition Size\n",
        "(80 * 128MB ≈ 10GB total partition spread)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VH5yl7ZatxTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_result = transactions_pruned.groupBy(\"product_id\").agg(sum(\"amount\").alias(\"total_sales\"))\n",
        "final_result.coalesce(80).write.mode(\"overwrite\").parquet(\"final_sales_data\")\n"
      ],
      "metadata": {
        "id": "r-VoOkVjt0ML"
      },
      "execution_count": 50,
      "outputs": []
    }
  ]
}